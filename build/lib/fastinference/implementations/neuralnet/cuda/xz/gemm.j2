{#
 # Binary General Matrix Multiplication
 #}

// // Layer {{ layer_id }}: Gemm
// start = std::chrono::high_resolution_clock::now();
// {% if is_first_gemm_after_reshape == true %} {# not sure why I did this in the beginning#}
// for(int b = 0; b < {{batch_size}}; b++){
//   for (int d = 0; d < {{ layer.output_shape[1] }}; d++) {
//     cuda_layer_{{ layer_id }}_output[b*{{ layer.output_shape[1] }} + d] = layer_{{ layer_id }}_bias[d];
//   }
//   for (int d = 0; d < {{ layer.output_shape[1] }}; d++) {
//     for (int i = 0; i < {{ (layer.input_shape[1] / binary_word_size)|round(method='ceil')|int }}; i++) {
//       cuda_layer_{{ layer_id }}_output[b*{{ layer.output_shape[1] }} + d] += 2 * {{ popcount }}(({{ uint_type }})~({{ uint_type }})(layer_{{ layer_id }}_weight[d][i] ^ cuda_layer_{{ layer_id - 1 }}_output[b*{{ (layer.input_shape[1] / binary_word_size)|round(method='ceil')|int }} + i])) - {{ binary_word_size }};
//     }
//   }
// }
// {% else %}
// for(int b = 0; b < {{batch_size}}; b++){
//   for (int d = 0; d < {{ layer.output_shape[1] }}; d++) {
//     cuda_layer_{{ layer_id }}_output[b*{{ layer.output_shape[1] }} + d] = layer_{{ layer_id }}_bias[d];
//   }
//   for (int d = 0; d < {{ layer.output_shape[1] }}; d++) {
//     for (int i = 0; i < {{ (layer.input_shape[1] / binary_word_size)|round(method='ceil')|int }}; i++) {
//       cuda_layer_{{ layer_id }}_output[b*{{ layer.output_shape[1] }} + d] += 2 * {{ popcount }}(({{ uint_type }})~({{ uint_type }})(layer_{{ layer_id }}_weight[d][i] ^ cuda_layer_{{ layer_id - 1 }}_output[b*{{ (layer.input_shape[1] / binary_word_size)|round(method='ceil')|int }} + i])) - {{ binary_word_size }};
//     }
//   }
// }
// {% endif %}
// end = std::chrono::high_resolution_clock::now();
// auto l{{ layer_id }}_time = static_cast<float>(std::chrono::duration_cast<std::chrono::nanoseconds>(end-start).count());  
// float l{{ layer_id }}_kernel_time = 0;
// 
// // checksum L{{ layer_id }} 
// ofstream g{{ layer_id }}("outputs/layer{{ layer_id }}/orig.out");
// for(int b = 0; b < {{ batch_size }}; b++){
//   sum_cpu = 0;
//   for (int d = 0; d < {{ layer.output_shape[1] }}; d++) {
//     sum_cpu += cuda_layer_{{ layer_id }}_output[b*{{layer.output_shape[1]}} + d];
//     g{{ layer_id }}<<cuda_layer_{{ layer_id }}_output[b*{{layer.output_shape[1]}} + d]<<" ";  
//   }
//   cout<<fixed<<"layer {{ layer_id }}(CPU): batch "<<b<<": "<<sum_cpu<<endl;
// }
// cout<<endl;

/* Layer {{ layer_id }} GPU */ 
start = std::chrono::high_resolution_clock::now();
float l{{ layer_id }}_kernel_time = layer{{ layer_id }}_gpu(cuda_layer_{{ layer_id-1 }}_output, cuda_layer_{{ layer_id }}_output);
end = std::chrono::high_resolution_clock::now();  
auto l{{ layer_id }}_time = static_cast<float>(std::chrono::duration_cast<std::chrono::nanoseconds>(end-start).count());
l{{ layer_id }}_time -= l{{ layer_id }}_kernel_time*1000000.0f; // ms->ns
kernel_time += l{{ layer_id }}_kernel_time;

// // checksum L{{ layer_id }}
// ofstream gg{{ layer_id }}("outputs/layer{{ layer_id }}/par.out");
// for(int b = 0; b < {{ batch_size }}; b++){
//   sum_gpu = 0;
//   for(int i=b*{{ layer.output_shape[1] }};i<(b+1)*{{ layer.output_shape[1] }};i++){
//     sum_gpu += cuda_layer_{{ layer_id }}_output[i];
//     gg{{ layer_id }}<<cuda_layer_{{ layer_id }}_output[i]<<" ";  
//   }
//   cout<<fixed<<"layer {{ layer_id }}(GPU): batch "<<b<<": "<<sum_gpu<<endl;
// }
// cout<<endl;
