// layer {{layer_id}} maxpool

__global__ void layer{{layer_id}}_gpu_kernel({{input_type}} *d_cuda_layer_{{layer_id-1}}_output, {{output_type}} *d_cuda_layer_{{layer_id}}_output){
    
    int N = ({{input_shape[2]}}+1); // +1 to cover all edges (fixes bug #ky2)
    int kernel_size = {{kernel_shape[0]}};

    int tid = threadIdx.x; // = h
    int bid = blockIdx.y;  // = w
    int h = tid, w = bid;

    int c = blockIdx.z; // neurons in z-dir

    // batches in x-dir
    int b = blockIdx.x;
    //each block is assigned to a row of an image, iy index of y value                  
    int iy = blockIdx.y + (kernel_size - 1)/2;  
    //each thread is assigned to a pixel of a row, ix index of x value
    int ix = threadIdx.x + (kernel_size - 1)/2; 
    
    //idx global index (all blocks) of the image pixel 
    int idx = iy*N +ix;

    {% if layer.pads == [0, 0, 0, 0] %}
    // bias is applied to every pixel
    if(tid < N){
        if(b < {{batch_size}}){
            if(c < {{ (input_shape[1] / binary_word_size)|round(method='ceil')|int }})
            {
                d_cuda_layer_{{layer_id}}_output[index4D_cuda(b,h,w,c,{{output_shape[2]}},{{output_shape[3]}},{{ (input_shape[1] / binary_word_size)|round(method='ceil')|int }})] = 0;
            }
        }
    }

    __syncthreads();

    // edge pixels are skipped here because they cannot fit entire convolution window
    if(idx < N*N){
        for (int kH = 0; kH < {{kernel_shape[0]}}; kH++) {
            for (int kW = 0; kW < {{kernel_shape[1]}}; kW++) {
                if(b < {{batch_size}}){
                    if(c < {{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }})
                    {
                        d_cuda_layer_{{ layer_id }}_output[index4D_cuda(b,h,w,c,{{ output_shape[2] }},{{ output_shape[3] }},{{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }})] |= d_cuda_layer_{{ layer_id - 1 }}_output[index4D_cuda(b,(h * {{ layer.strides[0] }} + kH),(w * {{ layer.strides[1] }} + kW),c,{{ input_shape[2] }},{{ input_shape[3] }},{{ output_shape[1] }})];
                    }
                }
            }
        }
    }
    {% else %}
    // bias is applied to every pixel
    if(tid < N){
        if(b < {{batch_size}}){
            if(c < {{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }}) {
                d_cuda_layer_{{layer_id}}_output[index4D_cuda(b,h,w,c,{{output_shape[2]}},{{output_shape[3]}},{{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }})] = 0;
            }
        }
    }

    __syncthreads();

    // edge pixels are skipped here because they cannot fit entire convolution window
    if(idx < N*N){
        for (int kH = 0; kH < {{ kernel_shape[0] }}; kH++) {
            int iH = h * {{ layer.strides[0] }} + kH - {{ layer.pads[0] }};
            if (iH >= 0 && iH < {{ input_shape[2] }}) {
                for (int kW = 0; kW < {{ kernel_shape[1] }}; kW++) {
                    int iW = w * {{ layer.strides[1] }} + kW - {{ layer.pads[1] }};
                    if (iW >= 0 && iW < {{ input_shape[3] }}) {
                        if(b < {{batch_size}}){
                            if (c < {{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }}) {
                                d_cuda_layer_{{ layer_id }}_output[index4D_cuda(b,h,w,c,{{ output_shape[2] }},{{ output_shape[3] }},{{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }})] |= d_cuda_layer_{{ layer_id - 1 }}_output[index4D_cuda(b,iH,iW,c,{{ input_shape[2] }},{{ input_shape[3] }},{{ output_shape[1] }})];
                            }
                        }
                    }
                }
            }
        }
    }

    {% endif %}

}

float layer{{layer_id}}_gpu_cuda({{input_type}} * cuda_layer_{{layer_id-1}}_output, {{output_type}} * cuda_layer_{{layer_id}}_output){
    //setUniGPU(); // use the second GPU on Uni-server because the first is used most of the time

    // flatten 3D -> 1D arrays
    // no 3D arrays to be flattened

    // prepare for kernel call
    // declare storage on device
    {{input_type}} *d_cuda_layer_{{layer_id-1}}_output; // storage on device for cuda_layer_{{layer_id-1}}_output
    {{output_type}} *d_cuda_layer_{{layer_id}}_output; // RESULT storage on device for cuda_layer_{{layer_id}}_output

    // allocate GPU device buffers
    // Note: batch_size included in input and output shapes
    cudaMalloc((void **) &d_cuda_layer_{{layer_id-1}}_output, {{ input_shape|join('*') }}*sizeof({{input_type}})); // dim of cuda_layer_{{layer_id-1}}_output
    cudaMalloc((void **) &d_cuda_layer_{{layer_id}}_output, {{ output_shape|join('*') }}*sizeof({{output_type}})); // dim of layer_{{layer_id}}_output
    cudaCheckErrors("Failed to allocate device buffer");

    // copy input data from host on device
    cudaMemcpy(d_cuda_layer_{{layer_id-1}}_output, cuda_layer_{{layer_id-1}}_output, ({{ input_shape|join('*') }}*sizeof({{input_type}})), cudaMemcpyHostToDevice);

    // define thread and block sizes
    const int BLKXSIZE = {{output_shape[2]}};
    const int BLKYSIZE = 1;
    const int BLKZSIZE = 1;
    const int GRIDXSIZE = {{batch_size}};
    const int GRIDYSIZE = {{output_shape[3]}};
    {% if layer.pads == [0, 0, 0, 0] %}
    const int GRIDZSIZE = {{ (input_shape[1] / binary_word_size)|round(method='ceil')|int }};
    {% else %}
    const int GRIDZSIZE = {{ (output_shape[1] / binary_word_size)|round(method='ceil')|int }};
    {% endif %}

    const dim3 threadsPerBlock(BLKXSIZE, BLKYSIZE, BLKZSIZE);
    const dim3 numBlocks(GRIDXSIZE, GRIDYSIZE, GRIDZSIZE);

    // timing of the kernel
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    float milliseconds = 0;

    // compute result - kernel call
    cudaEventRecord(start);
    layer{{layer_id}}_gpu_kernel<<<numBlocks,threadsPerBlock>>>(d_cuda_layer_{{layer_id-1}}_output, d_cuda_layer_{{layer_id}}_output);
    cudaCheckErrors("Kernel launch failure");
    cudaEventRecord(stop);

    // synchronize threads
    cudaDeviceSynchronize();
    cudaCheckErrors("CUDA synchronize failure");
    cudaEventElapsedTime(&milliseconds, start, stop);

    // copy result from device to host
    cudaMemcpy(cuda_layer_{{layer_id}}_output, d_cuda_layer_{{layer_id}}_output, ({{ output_shape|join('*') }}*sizeof({{output_type}})), cudaMemcpyDeviceToHost);
    cudaCheckErrors("CUDA memcpy failure");

    // free the memory
    cudaFree(d_cuda_layer_{{layer_id-1}}_output);
    cudaFree(d_cuda_layer_{{layer_id}}_output);
    cudaCheckErrors("cudaFree fail");

    return milliseconds;
}

