// layer {{layer_id}} gemm yz

__global__ void layer{{layer_id}}_gpu_kernel({{input_type}} *d_cuda_layer_{{layer_id-1}}_output, {{bias_data_type}} *d_layer_{{layer_id}}_bias, {{weight_data_type}} *d_cuda_layer_{{layer_id}}_weight, {{output_type}} *d_cuda_layer_{{layer_id}}_output){

    int z = blockDim.x * blockIdx.z + threadIdx.x;
    int y = blockDim.y * blockIdx.y + threadIdx.y;

    int d = z*blockDim.x+y;

    if(d < {{output_shape[1]}}){
        for (int b = 0; b < {{batch_size}}; b++){
            d_cuda_layer_{{layer_id}}_output[b * {{output_shape[1]}} + d] = d_layer_{{layer_id}}_bias[d];
            for(int i = 0; i < {{ (input_shape[1] / binary_word_size)|round(method='ceil')|int }}; i++){
                d_cuda_layer_{{layer_id}}_output[b*{{output_shape[1]}} + d] += 2 * {{popcount_cuda}}(({{uint_type}})~({{uint_type}})(d_cuda_layer_{{layer_id}}_weight[d*{{(input_shape[1] / binary_word_size)|round(method='ceil')|int}} + i] ^ d_cuda_layer_{{layer_id-1}}_output[b*{{(input_shape[1] / binary_word_size)|round(method='ceil')|int}} + i])) - {{binary_word_size}};
            }
        }
    }   
}

float layer{{layer_id}}_gpu_cuda({{input_type}} * cuda_layer_{{layer_id-1}}_output, {{output_type}} * cuda_layer_{{layer_id}}_output){

    // setUniGPU(); // use the second GPU on Uni-server because the first is used most of the time

    // flatten 3D -> 1D arrays
    // flatten layer_{{layer_id}}_weight
    {{weight_data_type}} *cuda_layer_{{layer_id}}_weight = ({{weight_data_type}} *) layer_{{layer_id}}_weight;

    // prepare for kernel call
    // declare storage on device
    {{input_type}} *d_cuda_layer_{{layer_id-1}}_output; // storage on device for cuda_layer_{{layer_id-1}}_output
    {{bias_data_type}} *d_layer_{{layer_id}}_bias; // storage on device for layer_{{layer_id}}_bias
    {{weight_data_type}} *d_cuda_layer_{{layer_id}}_weight; // storage on device for cuda_layer_{{layer_id}}_weight
    {{output_type}} *d_cuda_layer_{{layer_id}}_output; // RESULT storage on device for cuda_layer_{{layer_id}}_output

    // allocate GPU device buffers
    // Note: batch_size included in input and output shapes
    cudaMalloc((void **) &d_cuda_layer_{{layer_id-1}}_output, {{batch_size}}*{{ input_shape|join('*') }}*sizeof({{input_type}})); // dim of cuda_layer_{{layer_id-1}}_output
    cudaMalloc((void **) &d_layer_{{layer_id}}_bias, {{bias_shape[0]}}*sizeof({{bias_data_type}})); // dim of layer_{{layer_id}}_bias
    cudaMalloc((void **) &d_cuda_layer_{{layer_id}}_weight, {{ weight_shape|join('*') }}/{{binary_word_size}}*sizeof({{weight_data_type}})); // dim of layer_{{layer_id}}_weight
    cudaMalloc((void **) &d_cuda_layer_{{layer_id}}_output, {{batch_size}}*{{ output_shape|join('*') }}*sizeof({{output_type}})); // dim of layer_{{layer_id}}_output
    cudaCheckErrors("Failed to allocate device buffer");

    // copy input data from host on device
    cudaMemcpy(d_cuda_layer_{{layer_id-1}}_output, cuda_layer_{{layer_id-1}}_output, ({{batch_size}}*{{ input_shape|join('*') }}*sizeof({{input_type}})), cudaMemcpyHostToDevice);
    cudaMemcpy(d_layer_{{layer_id}}_bias, layer_{{layer_id}}_bias, ({{bias_shape[0]}}*sizeof({{bias_data_type}})), cudaMemcpyHostToDevice);
    cudaMemcpy(d_cuda_layer_{{layer_id}}_weight, cuda_layer_{{layer_id}}_weight, ({{ weight_shape|join('*') }}/{{binary_word_size}}*sizeof({{weight_data_type}})), cudaMemcpyHostToDevice);
    cudaCheckErrors("CUDA memcpy failure");

    // define thread and block sizes
    /*
        Maximum threads in a block: 1024 => Maximum block size 32x32
        if more than 1024 threads are needed, then set block size to maximum (32x32) and put multiple blocks in z-dir
        else if less than 1024 are needed, then only create 1 (square) block in z-dir, of size ceil(sqrt(THREADS_NEEDED))
    */
    const int BLKXSIZE = std::ceil(sqrt({{ output_shape[1] }}));
    const int BLKYSIZE = std::ceil(sqrt({{ output_shape[1] }}));
    const int BLKZSIZE = 1;
    const int GRIDXSIZE = 1;
    const int GRIDYSIZE = 1;
    const int GRIDZSIZE = {% if output_shape[1] > 1024 %}std::ceil({{output_shape[1]}}/1024){% else %}1{% endif %};

    const dim3 threadsPerBlock(BLKXSIZE, BLKYSIZE, BLKZSIZE);
    const dim3 numBlocks(GRIDXSIZE, GRIDYSIZE, GRIDZSIZE);

    // timing of the kernel
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    float milliseconds = 0;

    // compute result - kernel call
    cudaEventRecord(start);
    layer{{layer_id}}_gpu_kernel<<<numBlocks,threadsPerBlock>>>(d_cuda_layer_{{layer_id-1}}_output, d_layer_{{layer_id}}_bias, d_cuda_layer_{{layer_id}}_weight, d_cuda_layer_{{layer_id}}_output);
    cudaCheckErrors("Kernel launch failure");
    cudaEventRecord(stop);

    // synchronize threads
    cudaDeviceSynchronize();
    cudaCheckErrors("CUDA synchronize failure");
    cudaEventElapsedTime(&milliseconds, start, stop);

    // copy result from device to host
    cudaMemcpy(cuda_layer_{{layer_id}}_output, d_cuda_layer_{{layer_id}}_output, ({{batch_size}}*{{ output_shape|join('*') }}*sizeof({{output_type}})), cudaMemcpyDeviceToHost);
    cudaCheckErrors("CUDA memcpy failure");

    // free the memory
    cudaFree(d_cuda_layer_{{layer_id-1}}_output);
    cudaFree(d_layer_{{layer_id}}_bias);
    cudaFree(d_cuda_layer_{{layer_id}}_weight);
    cudaFree(d_cuda_layer_{{layer_id}}_output);
    cudaCheckErrors("cudaFree fail");

    return milliseconds;
}